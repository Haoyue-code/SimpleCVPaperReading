# Objects as Points

## 摘要

检测任务将目标视为图片中的与坐标对齐的框。大多数成功的目标检测器都详尽的枚举潜在的目标位置，然后对每个位置进行分类。这种方法是浪费的、低效的、需要额外的后处理工作。在这篇文章中，我们另辟蹊径，将目标建模为一个单独的点-即目标框的中心点。本文提出的检测器使用**关键点估计**来寻找中心点，然后回归出目标的其他属性，比如大小、3D位置、方向、姿态等。基于中心点的方法，CenterNet，是一个端到端的、可微分的检测器，要比同等条件下基于目标框的检测器更简单，更快速，更准确。在MSCOCO数据及上，CenterNet能达到更好做到速度-精度权衡。142FPS速度实现了28.1%的AP，52FPS的速度实现了37.4%AP，1.4FPS的速度(多尺度测试)实现了45.1%的AP。在KITTI基准上以相同的方法来估计3D检测，在COCO关键点数据集上估计人体姿态，我们的方法与传统的多阶段方法相比，更具有竞争优势并且可以实时运行。

## 1. 介绍

目标检测驱动了很多视觉任务比如实力分割、姿态估计、跟踪、动作识别，也存在许多下游的应用比如视频监控、自动驾驶、视觉问答。目前，目标检测器使用的是紧紧围绕目标的一个坐标对齐的外包矩形框。这种方法将目标检测简化为大量潜在的物体外包矩形框的图像分类问题。对于每个外包矩形框来说，分类器决定了这个图像的内容是前景还是背景。

单阶段检测器将一系列可能的外包矩形框（也就是锚框）的复杂排列在图像中进行划窗，直接对其进行分类而不需要关注对应框中的内容。两阶段的检测器则为每个潜在的框重新计算了图像特征，然后分类这些特征。后处理也称之为非极大抑制算法会移除同一个个体的重复的检测框。后处理是不可微分的、不可训练，因此大多数目标检测器并不是可以端到端的训练。然而，过去五年中，这个想法达到了经验上的成功。基于划窗的目标检测器有一些浪费，因为他们需要枚举所有的目标可能存在的位置和维度。

在这篇文章中，我们提供了一个更加简单更加有效的选择。我们使用目标中心点作为一个目标的代表。其他属性，比如目标的大小，维度，3D区间，朝向和姿态等信息可以直接通过图像特征进行回归。目标检测在这种情况下就更类似一个标准的关键点估计问题。我们将输入图片喂到全卷积网络从而输出一个热度图。热图的峰值代表目标的中心点。每个峰值的图像特征会负责预测目标框的高和宽。魔性的训练是通过密集的有监督训练。推理过程是一个单网络的前向传播，不需要非极大抑制算法进行后处理。

![我们将目标框建模成一个点。目标框的大小和其他属性信息可以通过中心点的特征进行推断](CenterNet-Object%20As%20Points.assets/image-20200528140028620.png)

我们的方法具有普适性，可以以很小的代价扩展到其他任务中。我们在3D目标检测和多人姿态估计进行了试验，这些任务中不同的就是除了中心点以外的额外的输出。对于3D检测框估计问题来说，我们需要回归目标的绝对深度，3D检测框维度和目标朝向。对于人体姿态估计，我们将二维关节点位置看做来自中心的偏移量，直接回归到中心点位置。

CenterNet非常简单，能以非常高的速度运行。通过一个简单的ResNet18层和上采样鞥，我们的网络可以在COCO上以142FPS的速度达到28.1%的AP。通过精心设计的关键点检测网络DLA-34，能够在COCO上以52FPS的速度达到37.4%的AP。如果使用SOTA关键点估计网络Hourglass-104，并且结合上多尺度测试，我们的玩了个可以在COCO上以1.4FPS的速度达到45.1%。在3D 检测框估计的问题和人体姿态估计方面，我们以更高的推理速度可以和最新的技术进行竞争。

代码开源在：https://github.com/xingyizhou/CenterNet

## 2. 相关工作

**基于区域分类的目标检测算法** 最初最为成功的深度目标检测器R-CNN是通过从大量的候选区域中枚举目标的位置，然后将对应区域的图片抠出来，然后使用深度神经网络进行分类。Fast-RCNN则直接扣取对应区域的特征来节省计算量。然后，两种方法都依赖于缓慢的底层区域建议的方法。



**隐含对象的目标检测算法** Faster RCNN在检测网络中产生区域建议。 它在一个低分辨率的图像网格周围采样固定形状的边界框(锚) ，并分类为“前景或非前景”。 如果重叠IOU>0.7, 锚标被标记为前景，如果背景重叠0.3，则被认为是背景忽略。 每个生成的区域方案再次进行分类。如果将提出的分类器转化为多分类，这就构成了单阶段检测器的基础。Anchor先验、不同的特征图分辨率、损失函数重权衡等都是一阶段检测器的改进点。

我们的方法和基于锚框的一阶段方法非常相关。中心点可以被视为一个形状不可知的锚框。然后，这里有几个非常重要的区别。首先，CenterNet仅仅基于位置分配Anchor，而不是基于检测框重叠率。这里不需要人为设置阈值来作为前景背景的判别阈值。第二，每个物体只有一个正锚框，因此也不需要非极大抑制进行后处理。我们仅仅从关键点热图上提取局部极值。第三，CenterNet使用更高的输出分辨率（stride=4）,传统目标检测器通常输出分辨率较低（stride=16）。这就不需要多个锚框了。

**基于关键点估计的目标检测算法** 我们并不是第一个使用关键估计方法进行目标检测的。CornerNet检测两个目标框的对角作为关键点。ExtremeNet则检测目标框的上，左，下，右和中心点。这些方法都是采用了与CenterNet相同的鲁棒的关键点估计网络。然而，这些方法需要组合分组阶段，这会显著降低每个方法的速度。CenterNet简单地提取单个目标的中心点而不需要后处理。

**单目3D目标检测** 3D边界框的估计为自动驾驶应用提供了驱动力。Deep3Dbox使用了慢速RCNN的框架，首先检测2D对象，然后将每个对象输入3D估计网络。3D RCNN给Faster RCNN增加了一个额外的头部，然后增加了3D投影。Deepmanta使用了一个从粗到细的Faster RCNN来训练很多任务。我们的方法类似于Deep3Dbox或者3D RCNN的一阶段版本。因此，CenterNet比其他竞争方法要更简单并且更快。

![（a）基于锚框的检测器和（b）CenterNet](CenterNet-Object%20As%20Points.assets/image-20200529150041553.png)

## 3. 初步调研

$I\in R^{W\times H\times 3}$代表输入图片的宽度为W,高度为H。我们的目标是生成一个关键点热度图$\hat{Y}\in [0,1]^{\frac{W}{R}\times \frac{H}{R}\times C}$, 其中R是输出的跨步，C代表关键点类别的个数。在人体姿态关键点估计问题中，C=17。在目标检测问题中的类别领域，C=80.我们使用默认的stride=4.输出的步长用一个因子R对输出预测进行下采样。预测值$\hat{Y}_{x,y,c}=1$对应的是被检测到的关键点；$\hat{Y}_{x,y,c}=0$代表该位置是背景。我们使用了几个不同的去案卷集，编码解码格式的网络来从图像I中预测$\hat{Y}$：

- Hourglass Network
- 上卷积残差网络ResNet
- 深层聚集网络DLA

对于每个标注的类别$c$的关键点$p\in R^2$，我们计算出来一个低分辨率的等价量$\tilde{p}=\lfloor \frac{p}{R}\rfloor$。然后使用高斯核$Y_{xyz}=exp(-\frac{(x-\tilde{p_x})^2+(y-\tilde{p}_y)^2}{2\sigma^2_p})$将关键点散布到热图$Y\in[0,1]^{\frac{W}{R}\times \frac{H}{R} \times C}$。其中$\sigma_p$是一个目标大小自适应的标准偏差。如果相同的两个类别的高斯分布重合了，那就取其中最大的值作为最终结果。训练的目标就是减少关键点损失使用focal loss进行的像素级逻辑回归。

![](CenterNet-Object%20As%20Points.assets/image-20200529153729273.png)

其中$\alpha, \beta$是focal loss的超参数，N代表图像I中的关键点个数。选择N的标准化来讲所有的正样的focal loss的值设置为1。在这个实验中我们使用$\alpha=2, \beta=4$。为了恢复由于输出不符引起的离散化误差，额外为每个中心点都预测了一个局部偏移值$\hat{O}\in R^{\frac{W}{R}\times \frac{H}{R}\times 2}$。所有的类别C共享同一个偏移，偏移量使用的是L1损失函数进行训练：
$$
L_{off}=\frac{1}{N}\sum_{p}|\hat{O}_{\tilde{p}-(\frac{p}{R}-\tilde{p})}|
$$
监督仅仅作用在关键点坐标$\tilde{p}$，其他所有的位置会被忽视。

在下一节中，我们将展示如何将这个关键点估计器如何扩展到一个通用的对象检测器。





## 4. 目标作为点







## 5. 实现细节











## 6. 实验





## 7. 结论