# 【神经网络搜索】Single Path One Shot

【GiantPandaCV导读】Single Path One Shot(SPOS)是旷视和清华、港科大联合的工作。与之前的工作不同，SPOS可以直接在大型数据集ImageNet上搜索，并且文章还提出了一种缓和权重共享的NAS的解耦策略，让模型能有更好的排序一致性。

![](https://img-blog.csdnimg.cn/20210406084948885.png)



## 摘要

之前的One-Shot NAS训练难度很大，并且在大型数据集比如ImageNet上非常低效。SPOS就是来解决训练过程中的挑战，其核心思想构建一个简化的超网，每个结构都是单路径的，每次训练是一个单路径的子网络，通过这种方式可以缓解权重耦合的问题。训练过程中使用的是均匀路径采样，这样所有的子网的权重才能被充分且公平地得到训练。

SPOS训练方式简单并且搜索非常迅速，支持多重搜索空间比如block-wise， channel-wise，混合精度量化和资源受限情况下的搜索等，并且在ImageNet上实现了SOTA。



## 简介

目前的神经网络搜索方法可以分成以下几种：

- 使用嵌套优化的方式处理权重优化和网络架构优化的问题。
- 使用权重共享策略来降低计算量，加速搜索过程。
- 使用基于梯度的方法，将离散空间松弛到连续空间。
- 使用嵌套联合优化方法。

在基于梯度的方法中，存在一些问题：

- 超网中的权重是紧密耦合的，尚不清楚子网的权重继承为何是有效的。
- 使用同时优化的方式也给网络架构参数和超网参数引入了耦合。

基于梯度的方法在优化过程中可能会引入bias，从而误导网络的搜索，出现马太效应，算子被训练的次数越多，权重会越大，强者越强。

本文主要贡献：

- 对现有的NAS算法进行详尽的分析，并指出了现有的使用嵌套优化方法存在的缺点。
- 提出了均匀采样的single path one-shot方法，可以克服现有one-shot方法的缺点。其简单的形式允许更大的搜索空间，包括通道搜索、比特宽度搜索等。采用进化算法来进行搜索，可以满足低延迟等约束。



## 回顾以往的NAS方法

早期的NAS方法采用**嵌套优化**的方式来实现，第一步优化是优化子网络的权重，优化目标是降低在训练集上的loss；第二步优化是网络架构优化，所有子网中验证集上准确率最高的那个网络。这种方式**最大的缺点**是训练代价太大，很多工作只能在Cifar10这样的小数据集或者小的搜索空间中完成。

近来的NAS方法通常会采用**权重共享**的训练策略，在这种策略中，所有子网会继承超网的权重，这样就可以不从头开始训练，降低搜索代价。这样的策略可以在ImageNet这类大型数据集上进行快速搜索。

大多数权重共享方法将离散的搜索空间转化为连续的搜索空间，这样就可以使用**梯度优化**的方式来建模神经网络搜索问题。权重和架构参数是同时优化的，或者使用两级优化方法来处理。

基于权重共享方法的NAS有两个缺点：

- 超网中各个子网耦合度高，尚不清楚为何从超网继承权重的方式是有效的。
- 同时优化网络权重参数W和架构参数θ会不可避免对架构引入某些偏好，这样在优化过程中会偏向于训练某些权重，造成不公平训练。

