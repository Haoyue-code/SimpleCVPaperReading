用于自动驾驶的混合稀疏-稠密单目SLAM系统

## 0. 引言

随着自动驾驶系统的推广，越来越多的应用要求SLAM具备高动态定位以及稠密建图能力。在论文"A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving"中，作者提出了一种混合稀疏特征和稠密建图的SLAM系统，并利用UnRectDepthNet来进行单目相机的深度估计。重要的是，算法已经开源。

## 1. 论文信息

标题：A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving

作者：Louis Gallagher, Varun Ravi Kumar, Senthil Yogamani, John B. McDonald

来源：2021 Computer Vision and Pattern Recognition (CVPR)

原文链接：https://arxiv.org/abs/2108.07736

代码链接：https://github.com/robotvisionmu/DenseMonoSLAM

## 2. 摘要

在本文中，我们介绍了一个使用安装在移动车辆上的单目相机，逐步重建室外环境几何图形的稠密3D模型的系统。稠密模型提供了丰富的环境表示，有助于更高层次的场景理解、感知和规划。

我们的系统采用稠密深度预测和混合建图架构，在一个集成框架内结合了最先进的基于稀疏特征和稠密融合的视觉SLAM算法。

我们的新贡献包括混合稀疏-稠密相机跟踪和回环的设计，以及稠密深度预测中的尺度估计改进。我们使用来自稀疏方法的运动估计来克服户外车辆场景中典型的大且可变的帧间位移。然后，我们的系统使用整体图像对齐将活跃图像与稠密模型配准。这使得能够将实时帧和稠密深度预测融合到模型中。稀疏和稠密模型之间的全局一致性和对齐是通过直接在稠密模型的变形内应用来自稀疏方法的姿态约束来实现的。我们提供了轨迹估计和表面重建精度的定性和定量结果，展示了在KITTI数据集上的竞争性能。所提出方法的定性结果可在<https://youtu.be/Pn2uaVqjskY>查看。

## 3. 算法分析

如图1所示是作者提出的混合SLAM框架原理，其采用一种混合的方法来进行稠密的单目跟踪和建图。首先使用基于特征点法的ORB-SLAM3，为每一帧提供相机姿态的初始估计。然后，系统遵循稠密交替架构，扩展了ElasticFusion。

其中地图首先保持固定，同时使用上一步中的初始姿态估计值，对相机进行跟踪。一旦估计了相机姿态，将当前帧融合到地图中。作者使用SOTA自监督卷积神经网络UnRectDepthNet来稠密地预测深度估计。此外，各种子系统在不同的处理器上运行：稠密交替和深度预测网络使用GPU运行，而ORB-SLAM3在CPU上运行。作者提出的混合架构总结如下:

(1) 深度预测网络用于估计每一帧的度量深度图。同时，使用ORB-SLAM3的基于特征的跟踪算法进行相机运动的初始估计，该跟踪算法适合于车辆的快速运动。

(2) 通过将初始姿态估计与相机视野中的当前活跃模型对准，进一步细化初始姿态估计。

(3) 实时RGB图像和相应的预测度量深度图被融合到场景的全局稠密模型中。按照原始EF算法，surfel模型被分成活跃和非活跃部分。

(4) 当ORB-SLAM识别出一个闭环时，使用EF变形图中的结果环闭合约束来校正稠密表面的几何形状。这使得先前访问的地图的非活跃部分重新与当前活跃部分对齐。重要的是，这也使ORB-SLAM和EF的不同地图和相机轨迹保持一致。

![](https://img-blog.csdnimg.cn/15951c58ab8144bc9337ad0b89c997fc.png)

图1 混合SLAM框架概述

作者的主要贡献总结如下：

(1) 开发了一个单目SLAM算法，可以应用于室外环境中的移动车辆。据作者所述，这是第一个在自动驾驶场景中定量评估的完全稠密SLAM系统。

(2) 在混合架构中使用了稠密的深度预测网络，以松耦合的方式结合了最先进的基于稀疏特征跟踪和稠密融合的视觉SLAM算法。同时改进了以前的稠密深度预测方法，在SLAM应用中增加新的正则化损失和更好的尺度估计。

(3) 使用来自稀疏方法的运动估计来克服户外车辆场景中典型的大且可变的帧间位移，然后使用将实况图像与稠密模型配准对齐。稀疏和稠密模型之间的全局一致性和对齐是通过直接在稠密模型的变形内应用来自稀疏方法的姿态约束来实现的。

(4) 在KITTI基准数据集上评估了该方法，提供了轨迹和表面重建精度的定性和定量结果。

### **3.1 尺度感知深度估计**

基于UnRectDepthNet网络，作者为自监督深度估计建立了相同的SFM框架。并采用针孔相机投影模型进行视图合成，最终目标由光度项*L~p~*和边缘平滑正则项*L~s~*组成。采用了跨序列深度一致性损失*L~dc~*和尺度恢复方法。

此外，通过结合自动编码器来获得场景的鲁棒全局特征，设置区别性损失*L~dis~*以及收敛性损失*L~cvt~*。*L~dis~*和*L~cvt~*的主要目标是防止优化目标被困在诸如天空和道路等低纹理区域的几个局部最小值。最终总目标深度为：

<img src="https://img-blog.csdnimg.cn/dc9d2c9d09854dd0b764b6c1f36474a2.png" style="zoom: 40%;" />

此外，尺度模糊是单目深度估计中一个具有挑战性的问题。因此，需要一个绝对值作为锚点，即通过从另一个专用传感器进行的测量来提供，以获得实际的深度估计。

在正文中，作者使用Velodyne点云和校准信息的结合来改进尺度的估计。并使用Velodyne激光雷达作为真值，通过将计算与图像平面上的正确像素关联来估计比例因子，进而最小化损失。此外，作者假设训练和测试数据集之间的深度一致性，它可以对具有高姿态可变性的数据集提供帮助。

### **3.2 基于特征的ORB-SLAM3-RGBD跟踪**

ORB-SLAM3建立了场景的稀疏地图，并使用共视图来表示。共视图中的每个节点对应一个关键帧，其由一个位姿和一组3D点组成。当两个关键帧之间存在共视关系时，它们之间的一条边会添加到共视图中。ORB-SLAM3有3个主线程。跟踪线程从相机接收RGB-D帧，提取ORB特征，并利用前一帧通过仅定位BA计算初始姿态估计。

通过将当前帧与可见关键帧的局部图对齐，进一步细化估计。检测到新的关键帧并将其发送到建图线程。此处，新关键帧附近的局部地图通过全局BA进行了优化。回环检测使用DBoW进行，如果新关键帧和匹配关键帧之间的几何对齐成功，则形成回环，并将两个关键帧之间的边添加到图形中。关键帧附近的局部地图被严格地变换到适当的位置。关键帧图的其余部分使用姿势图优化进行校正。执行最终的全局BA以恢复所有关键帧姿态和结构的MAP估计。

### **3.3 混合相机跟踪**

在混合系统中，首先将RGB图和预测得到的深度图传递给ORB-SLAM，以计算相机位姿的初始估计。之后需要平衡位姿的准确性，以及与稠密模型保持对齐，进而准确地融合关键帧。为了使相机重新与模型对齐，作者对位姿进行了帧到模型的细化。

在位姿周围的活跃地图被渲染成一个位于该位姿处的虚拟相机。然后，在嵌入3级图像金字塔的非线性最小二乘中，估计对准实时帧到虚拟帧的6自由度齐次变换矩阵。用位姿合成变换可以为当前帧产生一个精确的位姿估计。

### **3.4 混合回环**

如图3所示，混合回环有两个目标：(1) 调整稠密表面的几何形状以与现实世界保持一致；(2) 保持稠密地图与ORB-SLAM的稀疏建图和相机姿态估计一致。它还平衡了对这种一致性的需要和修正稠密几何的计算强度。

![](https://img-blog.csdnimg.cn/781a5bbf8f4d4b2f94be1dd45e394936.png)

图2 回环示例

(i) 一辆汽车开始探索(绿色箭头)；(ii) 在一段时间之后，由汽车先前绘制的区域与其当前位置之间的回环；

(iii) 在应用回环之前，漂移明显；(iv) 模型的活跃部分(绿色)偏离了模型的非活跃部分(灰色)；(v) 地图的活跃部分与非活跃部分对齐；(vi) 触发回环，重新激活用于建图和跟踪的部分；(vii) 最终的全局模型

## 4. 实验

作者主要在KITTI上进行测试，主要评估了轨迹估计和表面重建的精度。系统运行的硬件条件为i7−7700K CPU、16GB内存和NVIDIA GTX 1080Ti GPU。

### **4.1 KITTI-跟踪**

作者展示了KITTI数据集中序列01、02、06、08、09和10的结果，其余序列用于深度预测网络的训练。对于每个测试序列，使用长度从100米到800米的序列的相对平移误差。表1显示了实验方案和ORB-SLAM2以及D3VO的对比结果。

实验表明，使用稠密深度预测和ORB-SLAM(O)的RGBD模式，可以使用单目相机进行精确的度量尺度的相机跟踪。当引入混合跟踪(H)，它允许当前的帧被融合到模型中。引入混合回环(H+L)有助于使稀疏和稠密的模型恢复对齐，并减少模型和轨迹中的全局误差。此外，由于结构很少的场景会导致深度预测和相机跟踪的退化，因此序列01是一个重要挑战，图3显示了这个序列的定性结果。

表1 KITTI数据集上的相对平移误差

<img src="https://img-blog.csdnimg.cn/b702aa349b9d4dbf8519f280f534374e.png" style="zoom: 40%;" />

<img src="https://img-blog.csdnimg.cn/90e405b195a24b738b6bc42b13f0cccf.png" style="zoom:67%;" />

图3 KITTI数据集01序列的定性结果

### **4.2 KITTI-表面重建**

作者从KITTI数据集中评估了表面重建精度。由于KITTI不包括表面真值，作者将其与每个序列的Velodyne点云构建的模型进行比较。表2中显示了估计模型中的点与Velodyne点云中的最近点之间的表面到表面的平均距离。在计算分数之前，这两个模型都是严格对齐的。

表2 KITTI数据集上的表面精度

<img src="https://img-blog.csdnimg.cn/769e20cdf9f44958bfa0845903393fc6.png" style="zoom: 33%;" />

### **4.3 系统资源使用**

在图4中，作者展示了KITTI数据集中序列09的帧处理时间的细分。建图时间(蓝色)根据surfel数量(紫色)增加。该系统运行在8−9 Hz之间，序列末端的峰值是由于一个全局回环产生的。表3显示了在KITTI特征分割测试集上的运行性能的分布。

<img src="https://img-blog.csdnimg.cn/c2c0d380c05247f1a0e8a4136f5713f3.png" style="zoom: 67%;" />

图4 系统所花费的时间的细分

表3 KITTI特征分割的测试序列上的分布

<img src="https://img-blog.csdnimg.cn/304e0fb11ecc4bf485c1b2f1c1c7a370.png" style="zoom:45%;" />

### **4.4 KITTI-深度估计**

如表4所示，作者使用了KITTI特征分割上的深度估计设置。结果显示，作者提出的方案优于之前所有的单目自监督方法。根据最佳实践，作者将深度限制在80米，并使用改进的深度图真值进行评估。如图5所示是在KITTI数据集上的轨迹和稠密重建结果。

表4 对改进的KITTI特征分割的深度估计的评估

<img src="https://img-blog.csdnimg.cn/877217ec4a204869b76560fea9ef89e6.png" style="zoom: 33%;" />

<img src="https://img-blog.csdnimg.cn/11f7326b8ff44d0ba08c4e8ec080a363.png" style="zoom:80%;" />

图5 KITTI数据集上的轨迹和稠密重建结果

(a) 作者提出系统产生的最终模型 (b)与轨迹真值的对比 (c)车辆十字路口时重建的近距离图

## 5. 结论

在2021 CVPR论文"A Hybrid Sparse-Dense Monocular SLAM System for Autonomous Driving"中，作者提出了一种将稠密深度预测、稀疏特征跟踪和稠密叠加融合技术相结合的混合SLAM系统。该系统允许在自动驾驶场景中使用单目相机对室外场景进行实时稠密的度量重建。稀疏跟踪提供了相机姿态估计，能够在车辆速度下鲁棒地运行。所得到的姿态用于稠密融合跟踪步骤中，以初始化整个图像对齐细化过程。

通过视觉位置识别和稀疏系统的姿态约束来保持模型的全局一致性，并将其传递到稠密融合算法，与基于变形图的地图校正步骤集成。该系统是第一个基于稠密单目融合的视觉SLAM系统。此外，作者提到，虽然该文章的重点是自动驾驶，但通过对深度预测网络的再训练，该系统可以很容易地适应其他场景。
