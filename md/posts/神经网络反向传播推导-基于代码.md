---
title: 神经网络反向传播推导
date: 2019-10-11 14:54:46
tags: 
- pytorch
- 反向传播
categories:
- 深度学习
mathjax: true
---

> 前言：最近在看《深度学习之PyTorch实战计算机视觉》这本书，里边第6章中《搭建一个简单的神经网络》中有一部分代码看不懂，经过与学长的探讨，进行了手工推导，明白了为何代码是这样写的。

## 1. 代码展示

```python
import torch

x = torch.randn(1, 2)
y = torch.randn(1, 3)

w1 = torch.randn(2, 4)
w2 = torch.randn(4, 3)

epoch_n = 20
learning_rate = 1e-6

for epoch in range(epoch_n):
    h1 = x.mm(w1)
    h1 = h1.clamp(min=0)
    y_pred = h1.mm(w2)

    loss=(y_pred-y).pow(2).sum()
    print("epoch:%d, loss:%.3f" % (epoch, loss))

    grad_y_pred = 2*(y_pred-y)
    grad_w2 = h1.t().mm(grad_y_pred)

    grad_h = grad_y_pred.clone()
    grad_h = grad_h.mm(w2.t())
    grad_h.clamp_(min=0)
    grad_w1 = x.t().mm(grad_h)

    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```

> ps: 为了debug时候的简便，改动了x,y,w1,w2的Tensor形状

## 2. 推导过程

```mermaid
graph TD
x--w1--> h1
h1--w2-->y_pred
```

loss表达式如下：
$$
\hat{y}=x\times w_1 \times w_2
\\
loss = argmin(\sum{(\hat{y}-y)^2})
$$

第一步：

> ​    grad_y_pred = 2*(y_pred-y)

$$
\frac{\partial loss}{\partial \hat{y}}=2(\hat{y}-y)
$$

第二步：

> ​    grad_w2 = h1.t().mm(grad_y_pred)

$$
\frac{\partial loss}{\partial w_2} = \frac{\partial loss}{\partial \hat{y}}\times \frac{\partial \hat{y}}{\partial w_2}
\\
\frac{\partial \hat{y}}{\partial w_2} = \frac{\partial h_1w_2}{\partial w_2} = h_1^{T}
\\
\frac{\partial loss}{\partial w_2} = \frac{\partial loss}{\partial \hat{y}}\times \frac{\partial \hat{y}}{\partial w_2} = 2\times(\hat{y}-y)\times h_1^{T}
$$

与代码相符

第三步：

>  grad_h = grad_y_pred.clone()
>  grad_h = grad_h.mm(w2.t())

$$
\frac{\partial loss}{\partial h_1}=\frac{\partial loss}{\partial \hat{y}}\times \frac{\partial \hat{y}}{\partial h_1} = 2\times (\hat{y}-y)\times w_2^{T}
$$

第四步：

> ​    grad_w1 = x.t().mm(grad_h)

$$
\frac{\partial loss}{\partial w_1}=\frac{\partial loss}{\partial \hat{y}}\times \frac{\partial \hat{y}}{\partial w_1}=2\times(\hat{y}-y)\times w_2^{T}\times x^{T}
$$



## 3. 用到的公式

矩阵求导：

$$
\frac{\partial wx}{\partial x}=w^{T}
$$

---

目前只用到了这个公式，就可以解决，如果有不对的地方，希望大家指正。



