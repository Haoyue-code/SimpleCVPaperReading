

## 神经网络是基本的贝叶斯

> https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8

神经网络虽然在很多领域都取得了巨大的成功，但是缺乏对神经网络泛化性能理论上的理解。

Inductive biases 归纳偏差，它选择了一个代理认为最有可能解释一些训练数据的函数。一个好的归纳偏差需要保证泛化性能足够好。归纳偏差是一个关于机器学习算法目标函数的假设，通过学习样本，使得学习器对任意输入都能产生正确的预测。

对于像深度神经网络(已知能够表达许多可能的功能)这样的高表达性学习代理，需要一个良好的**归纳偏差**来保证泛化。

作者认为DNN和贝叶斯学习代理有相同的行为特征，都是基于参数的先验。这些先验偏好于简单的函数，这是DNN中良好的归纳偏差的来源。

很多理论显示，归纳偏差的主要来源是SGD。我们的实验是反对这一说法的有力证据，但确实表明调整随机优化器的超参数对训练后的DNNs有轻微的影响，并**可能放大先验中已经存在的对参数的偏差**。

![](https://img-blog.csdnimg.cn/20210401123421869.png)

上图展示的是多项式拟合和DNN之间的不同之处，随着多项式次数的提高，模型往往会偏向于复杂，因此发生过拟合，影响了模型的泛化能力。而DNN不同，它倾向于拟合简单的函数，虽然其有能力拟合红色的线，但是实际上它选择拟合的是蓝色的线。

> 这个实验证明，DNN的归纳偏好是朝向更简单的函数。奥卡姆剃刀原则

那么DNN归纳偏好的来源是什么？

















